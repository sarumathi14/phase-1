{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e09ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarum\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\airflow\\__init__.py:36: RuntimeWarning: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via https://github.com/apache/airflow/issues/10388, but it is not a high priority.\n",
      "  warnings.warn(\n",
      "OSError while attempting to symlink the latest log directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">C:\\Users\\sarum\\AppData\\Local\\Temp\\ipykernel_1204\\</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2181244185.</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">176</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mC:\\Users\\sarum\\AppData\\Local\\Temp\\ipykernel_1204\\\u001b[0m\u001b[1;33m2181244185.\u001b[0m\u001b[1;33mpy:\u001b[0m\u001b[1;33m176\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 3, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "def extract_new_ipl_matches():\n",
    "    \"\"\"\n",
    "    Extract new IPL matches from Cricbuzz that aren't already in our database\n",
    "    Returns a list of dictionaries containing match information\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.cricbuzz.com/cricket-series/7607/indian-premier-league-2024/matches\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get existing match URLs from database\n",
    "        pg_hook = PostgresHook(postgres_conn_id='postgres_default')\n",
    "        connection = pg_hook.get_conn()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT match_url FROM ipl_matches\")\n",
    "        existing_urls = {row[0] for row in cursor.fetchall()}\n",
    "        \n",
    "        # Scrape new matches from Cricbuzz\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        match_cards = soup.find_all('div', class_='cb-col-75 cb-col')\n",
    "        new_matches = []\n",
    "        \n",
    "        for card in match_cards:\n",
    "            match_info = {}\n",
    "            title_tag = card.find('a', class_='text-hvr-underline')\n",
    "            if title_tag:\n",
    "                match_url = \"https://www.cricbuzz.com\" + title_tag['href']\n",
    "                # Only process if match not already in database\n",
    "                if match_url not in existing_urls:\n",
    "                    match_info['title'] = title_tag.text.strip()\n",
    "                    match_info['match_url'] = match_url\n",
    "                    \n",
    "                    # Extract other match details\n",
    "                    series_info = card.find('div', class_='text-gray')\n",
    "                    if series_info:\n",
    "                        match_info['series_info'] = series_info.text.strip()\n",
    "                    \n",
    "                    location_time = card.find('div', class_='text-gray cb-font-12')\n",
    "                    if location_time:\n",
    "                        parts = [part.strip() for part in location_time.text.split('â€¢') if part.strip()]\n",
    "                        if len(parts) >= 2:\n",
    "                            match_info['venue'] = parts[0]\n",
    "                            match_info['date_time'] = parts[1]\n",
    "                    \n",
    "                    result_tag = card.find('div', class_='cb-scr-wll-chvrn cb-lv-scrs-col')\n",
    "                    if result_tag:\n",
    "                        result = result_tag.text.strip()\n",
    "                        # Only include completed matches\n",
    "                        if 'won' in result.lower() or 'abandoned' in result.lower():\n",
    "                            match_info['result'] = result\n",
    "                            new_matches.append(match_info)\n",
    "        \n",
    "        return new_matches\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extraction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def transform_match_data(new_matches):\n",
    "    \"\"\"\n",
    "    Transform raw match data into a structured format for database insertion\n",
    "    \"\"\"\n",
    "    transformed = []\n",
    "    \n",
    "    for match in new_matches:\n",
    "        # Parse date and time\n",
    "        date_str = match.get('date_time', '')\n",
    "        try:\n",
    "            match_date = datetime.strptime(date_str, '%b %d, %Y, %I:%M %p')\n",
    "            formatted_date = match_date.strftime('%Y-%m-%d')\n",
    "            formatted_time = match_date.strftime('%H:%M')\n",
    "        except ValueError:\n",
    "            formatted_date = date_str\n",
    "            formatted_time = ''\n",
    "        \n",
    "        # Extract teams from title\n",
    "        teams = []\n",
    "        title = match.get('title', '')\n",
    "        if 'vs' in title:\n",
    "            teams = [team.strip() for team in title.split('vs')]\n",
    "        \n",
    "        # Create transformed record\n",
    "        record = {\n",
    "            'match_title': title,\n",
    "            'team1': teams[0] if len(teams) > 0 else '',\n",
    "            'team2': teams[1] if len(teams) > 1 else '',\n",
    "            'series': match.get('series_info', ''),\n",
    "            'venue': match.get('venue', ''),\n",
    "            'match_date': formatted_date,\n",
    "            'match_time': formatted_time,\n",
    "            'result': match.get('result', ''),\n",
    "            'match_url': match.get('match_url', ''),\n",
    "            'ingestion_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        transformed.append(record)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "def load_to_data_warehouse(transformed_data):\n",
    "    \"\"\"\n",
    "    Load transformed data to PostgreSQL data warehouse\n",
    "    \"\"\"\n",
    "    if not transformed_data:\n",
    "        print(\"No new matches to load\")\n",
    "        return\n",
    "    \n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_default')\n",
    "    \n",
    "    # Create table if not exists\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ipl_matches (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        match_title TEXT,\n",
    "        team1 TEXT,\n",
    "        team2 TEXT,\n",
    "        series TEXT,\n",
    "        venue TEXT,\n",
    "        match_date DATE,\n",
    "        match_time TIME,\n",
    "        result TEXT,\n",
    "        match_url TEXT UNIQUE,\n",
    "        ingestion_timestamp TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    pg_hook.run(create_table_sql)\n",
    "    \n",
    "    # Insert new matches\n",
    "    for match in transformed_data:\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT INTO ipl_matches (\n",
    "            match_title, team1, team2, series, venue, \n",
    "            match_date, match_time, result, match_url, ingestion_timestamp\n",
    "        ) VALUES (\n",
    "            %(match_title)s, %(team1)s, %(team2)s, %(series)s, %(venue)s,\n",
    "            %(match_date)s, %(match_time)s, %(result)s, %(match_url)s, %(ingestion_timestamp)s\n",
    "        ) ON CONFLICT (match_url) DO NOTHING;\n",
    "        \"\"\"\n",
    "        pg_hook.run(insert_sql, parameters=match)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(transformed_data)} new matches\")\n",
    "\n",
    "def etl_ipl_matches():\n",
    "    \"\"\"\n",
    "    Complete ETL process for IPL matches\n",
    "    \"\"\"\n",
    "    new_matches = extract_new_ipl_matches()\n",
    "    if not new_matches:\n",
    "        print(\"No new completed matches found\")\n",
    "        return\n",
    "    \n",
    "    transformed_data = transform_match_data(new_matches)\n",
    "    load_to_data_warehouse(transformed_data)\n",
    "\n",
    "with DAG(\n",
    "    'ipl_daily_ingestion',\n",
    "    default_args=default_args,\n",
    "    description='Daily ETL pipeline for IPL match data from Cricbuzz',\n",
    "    schedule_interval='0 18 * * *',  # Runs daily at 6 PM UTC (11:30 PM IST)\n",
    "    catchup=False,\n",
    "    tags=['ipl', 'cricket', 'etl'],\n",
    ") as dag:\n",
    "    \n",
    "    extract_transform_load = PythonOperator(\n",
    "        task_id='etl_ipl_matches',\n",
    "        python_callable=etl_ipl_matches,\n",
    "    )\n",
    "\n",
    "    extract_transform_load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
